<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Beyond Bare Queries:Open-Vocabulary Object Grounding with 3D Scene Graph">
  <meta name="keywords" content="3D scene understanding, open-vocabulary object retrieval, scene graph, large language model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body>
<section class="authors">
  <div class="authors-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/linukc">Sergey Linok</a><sup>1&#134;</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/wingrune">Tatiana Zemskova</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Svetlana Ladanova<sup>1</sup>,</span>
            <span class="author-block">
              Roman Titkov<sup>1</sup>,</span>
            <span class="author-block">
              Dmitry Yudin<sup>1,2</sup>,</span>
            <span class="author-block">
              Maxim Monastyrnyy<sup>3</sup>,</span>
            <span class="author-block">
              Aleksei Valenkov<sup>3</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <sup>1</sup><span class="author-block">Moscow Institute of Physics and Technology (MIPT)</span>, 
            <sup>2</sup><span class="author-block">AIRI</span>,
            <sup>3</sup><span class="author-block">Sberbank of Russia, Robotics Center</span>
          </div>
          <div class="is-size-6 publication-authors">
            <sup>&#134;</sup><span class="author-block">means corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/linukc/BeyondBareQueries"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Locating objects referred to in natural language
          poses a significant challenge for autonomous agents. Existing
          CLIP-based open-vocabulary methods successfully perform 3D
          object grounding with simple (bare) queries, but cannot cope
          with ambiguous descriptions that demand an understanding
          of object relations. To tackle this problem, we propose a
          modular approach called BBQ (Beyond Bare Queries), which
          constructs 3D scene graph representation with metric and
          semantic edges and utilizes a large language model as a human-
          to-agent interface through our deductive scene reasoning al-
          gorithm. BBQ employs robust DINO-powered associations to
          construct 3D object-centric map and an advanced raycasting
          algorithm with a 2D vision-language model to describe them
          as graph nodes. On the Replica and ScanNet datasets, we
          have demonstrated that BBQ takes a leading place in open-
          vocabulary 3D semantic segmentation compared to other zero-
          shot methods. Also, we show that leveraging spatial relations
          is especially effective for scenes containing multiple entities
          of the same semantic class. On challenging Sr3D and Nr3D
          benchmarks, our deductive approach demonstrates a significant
          improvement, enabling objects grounding by complex queries
          compared to other state-of-the-art methods. The combination
          of our design choices and software implementation has resulted
          in significant data processing speed in experiments on the edge
          device. This promising performance enables the application of
          our approach in intelligent robotics projects. 
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<section class="Method">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
          <img src="./static/images/BBQ-GraphicalAbstract.png" 
               class="interpolation-image" 
               alt="Interpolate start reference image" 
               style="width: 80%; height: auto; display: block; margin: 0 auto;">
          <p class="video-description has-text-centered">Proposed BBQ approach leverages foundation models for high-
            performance construction of an object-centric class-agnostic 3D map of a
            static indoor environment from a sequence of RGB-D frames with known
            camera poses and calibration. To perform scene understanding, we represent
            environment as a set of nodes with spatial relations. Utilizing a designed
            deductive scene reasoning algorithm, our method enable efficient natural
            language interaction with a scene-aware large language model.</p>
          <img src="./static/images/BBQ-Scheme.png" 
               class="interpolation-image" 
               alt="Interpolate start reference image" 
               style="width: 100%; height: auto; display: block; margin: 0 auto;">
          <p class="video-description has-text-centered">An object-centric class-agnostic 3D map is iteratively constructed from a sequence of RGB-D camera frames and their poses by associating 2D
            SAM mask proposals with 3D objects with deep DINO visual features and spatial constraints (Sec. III-A). To visually represent objects after building the
            map, we select the best view based on the largest projected mask from K cluster centroids that represent areas of object observations (Sec. III-B). We
            leverage LLaVA to describe object visual properties (Sec. III-C). With the nodeâ€™s text descriptions, spatial locations, metric and semantic edges (Sec.
            III-D) we utilize LLM in our deductive reasoning algorithm (Sec. III-E) to perform a 3D object grounding task.</p>
</section>
<section class="Results">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
          <h1 class="title is-4">3D open-vocabulary semantic segmentation on Replica and ScanNet datasets</h2>
            <div class="content has-text-justified">
            <img src="./static/images/3DOVSemSeg_table.png" 
              class="interpolation-image" 
              alt="Interpolate start reference image" 
              style="width: 80%; height: auto; display: block; margin: 0 auto;">
            <img src="./static/images/3DOVSemSeg_Replica_room0.png" 
               class="interpolation-image" 
               alt="Interpolate start reference image" 
               style="width: 100%; height: auto; display: block; margin: 0 auto;">
          <!-- <h1 class="title is-4">3D referred object retrieval on the Sr3D/Nr3D datasets</h2>
            <div class="content has-text-justified">
            <img src="./static/images/3D_Retrieval_Nr3D_Sr3D_table.png" 
              class="interpolation-image" 
              alt="Interpolate start reference image" 
              style="width: 80%; height: auto; display: block; margin: 0 auto;">
            <img src="./static/images/QA_Sr3D_Nr3D.png" 
               class="interpolation-image" 
               alt="Interpolate start reference image" 
               style="width: 100%; height: auto; display: block; margin: 0 auto;"> -->
          <!-- <h1 class="title is-4">Time consumption analysis</h2>
            <div class="content has-text-justified">
            <p>We perform time consumption analysis on a machine with <i>Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz</i> and <i>Tesla V100-SXM2-32GB</i>.
              On Replica scenes with a mean of 30 detections per frame and 100 average number of objects, our pipeline runs with 1-1.5 fps per second for 3D object-centric map construction.
              In contrast, our closest analog ConceptGraphs requires 4-5 seconds to perform the same task per step. 
              We achieve such fast results due our design and fast components: MobileSAMv2 (~0.2s per frame), DINO embeddings (~0.12s per frame), GPU realization of DBSCAN noise removal (~0.005s per object),
              IOU-based spatial similarities, GPU-based clusterization to drastically reduce best view search space for 3D to 2D raycasting projection.</p> -->

<!-- </section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{linok2024bare,
      title={Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph}, 
      author={Sergey Linok and Tatiana Zemskova and Svetlana Ladanova and Roman Titkov and Dmitry Yudin},
      year={2024},
      eprint={2406.07113},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre>
  </div>
</section> -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The souce code is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>,
              we thank the authors for sharing the templates.
          </p>
        </div>
      </div>
      <a href="https://mapmyvisitors.com/web/1bw4q"  title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=C6tYBi-zUAcUjn0-KFJV1KaftFhTp2GrOlPaCdmIs9c&cl=ffffff" /></a>
    </div>
  </div>
</footer>
</body>

</html>
